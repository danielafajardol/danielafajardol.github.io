<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fun With Diffusion Models!</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
            color: #333;
        }
        header {
            background: #2f2e2e;
            color: #ffffff;
            padding: 10px 20px;
            text-align: center;
        }
        main {
            padding: 20px;
            max-width: 900px;
            margin: 0 auto;
            background: #fff;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            margin-top: 20px;
        }
        section {
            margin-bottom: 20px;
        }
        h1, h2, h3 {
            color: #444;
        }
        img {
            display: block;
            max-width: 100%;
            height: auto;
            margin: 10px 0;
        }
        code {
            background: #f4f4f4;
            padding: 2px 4px;
            border-radius: 4px;
            font-family: Consolas, monospace;
        }
        .images-row {
            display: flex;
            justify-content: center;
            gap: 20px;
            margin: 20px 0;
        }
    </style>
</head>
<body>
    <header>
        <h1>Fun With Diffusion Models!</h1>
        <p>By: Daniela Fajardo</p>
    </header>
    <main>
        <section>
            <h2>Overview</h2>
            <p>In this project, I implemented and deployed diffusion models for image generation.</p>
        </section>
        <section>
            <h2>Part A: Diffusion Models Exploration</h2>
            <h3>Overview</h3>
            <p>In part A, I explored diffusion models, implemented sampling loops, and applied them for tasks such as inpainting and creating optical illusions.</p>

            <h3>Part 0: Image Generation with DeepFloyd IF</h3>
            <p>My seed is <strong>3037150359</strong>.</p>
            <p>
                Using the DeepFloyd IF diffusion model, a two stage Diffusion Model trained by Stability AI, 
                I generated 3 images for each one of the 3 provided text prompts. I varied the number of inference  
                steps from 5, 10, 20 and 40 for each one of  the images. Below are the results.

                The generated images align with the provided prompts. 
                We can  see that as num_inference_steps increases, 
                the images become more detailed and “artistic”. 

            </p>
            
            <div class="images-row">
                <figure>
                    <img src="results_proj_5/num_inferences_5.png" alt="Inference Steps 5">
                    <figcaption>Inference Steps 5</figcaption>
                </figure>
                <figure>
                    <img src="results_proj_5/num_inferences_10.png" alt="Inference Steps 10">
                    <figcaption>Inference Steps 10</figcaption>
                </figure>
            </div>
            <div class="images-row">
                <figure>
                    <img src="results_proj_5/num_inferences_20.png" alt="Inference Steps 20">
                    <figcaption>Inference Steps 20</figcaption>
                </figure>
                <figure>
                    <img src="results_proj_5/num_inferences_40.png" alt="Inference Steps 40">
                    <figcaption>Inference Steps 40</figcaption>
                </figure>
            </div>

            <h3>1.1 Forward Process</h3>
            <p>
                In this step, I implemented the forward process of diffusion, which adds scaled noise 
                to a clean image at different timesteps. Using a test image, I visualized the results 
                for t=250,500,750t = 250, 500, 750t=250,500,750, showing how the image becomes progressively 
                noisier as t increases. These results demonstrate the gradual transformation of a clean image 
                into noise, setting the stage for the reverse process to reconstruct the original image.
            </p>
            <img src="results_proj_5/1.1.im.png" alt="Forward Process Visualization">

            <h3>1.2 Classical Denoising</h3>
            <p>I used Gaussian Blur to try to remove the noise</p>
            <div class="images-row">
                <figure>
                    <img src="results_proj_5/1.2.250_im.png" alt="Gaussian Blur 250">
                    <figcaption>Noisy Campanile at t=250</figcaption>
                </figure>
                <figure>
                    <img src="results_proj_5/1.2.500_im.png" alt="Gaussian Blur 500">
                    <figcaption>Noisy Campanile at t=500</figcaption>
                </figure>
                <figure>
                    <img src="results_proj_5/1.2.750_im.png" alt="Gaussian Blur 750">
                    <figcaption>Noisy Campanile at t=750</figcaption>
                </figure>
            </div>

            <h3>1.3 One-Step Denoising</h3>
            <p>In this step, I utilized a pretrained UNet from the diffusion model 
                to denoise images. Using the noisy images generated in Part 1.2 
                (t=250,500,750t = 250, 500, 750t=250,500,750), I:
                1. Estimated the Gaussian noise present in the noisy images using the UNet.
                2. Removed the noise to reconstruct an estimate of the original image.

                I visualized the results for each timestep, showing the progression from the clean 
                original image to the noisy version and then back to the denoised estimate. This 
                demonstrates the model's ability to recover meaningful details from noisy inputs, 
                providing a closer approximation to the original image.
                </p>
            
                <img src="results_proj_5/1.3.im1.png" alt="250">
                <img src="results_proj_5/1.3.im2.png" alt="500">
                <img src="results_proj_5/1.3.im3.png" alt="750">
            </div>

            <h3>1.4 Iterative Denoising</h3>
            <p>
                In this step, I worked on iterative denoising, a powerful technique used in diffusion 
                models to clean up noisy images step by step. Starting with a heavily distorted image,
                 I gradually refined it over multiple steps, following a schedule of decreasing noise 
                 levels. This approach proved much better at reconstructing detailed, high-quality 
                 images compared to simpler methods like single-step denoising or blurring.
            </p>
            <img src="results_proj_5/1.4.im.png" alt="Iterative Denoising Results" max_height=75%>

            <h3>1.5 Diffusion Model Sampling</h3>
            <p>
                In this step, I used the iterative denoising function to generate images from scratch 
                by starting with random noise. By setting the starting point to pure noise and 
                applying the denoising process iteratively, I was able to sample images corresponding 
                to the prompt "a high quality photo."
            </p>
            <img src="results_proj_5/1.5.im.png" alt="Generated Images">

            <h3>1.6 Classifier-Free Guidance</h3>
            <p>In this step, I implemented Classifier-Free Guidance (CFG) to significantly 
                enhance the quality of images generated from random noise. CFG works by 
                combining two noise estimates: one conditioned on a text prompt and another 
                unconditional estimate. By adjusting the balance between these estimates, 
                we can control the image quality.
                The images generated from scratch with the prompt "a high quality photo" and 
                CFG showed much higher quality compared to the previous section.
                </p>
            <img src="results_proj_5/1.6.im.png" alt="Classifier-Free Guidance Results">

            <h3>1.7 Image-to-Image Translation</h3>
            <p>
                In this step, I explored image-to-image translation, where I applied a noise and 
                denoising process to make gradual edits to an image. Starting with a real test image, 
                I added noise and used the iterative_denoise_cfg function to progressively refine the 
                image, simulating edits based on the prompt "a high quality photo." By adjusting the 
                noise levels, I created a series of images that became closer to the original, with 
                the edits becoming more refined as the noise level decreased.
            </p>
            <div class="images-row">
                <figure>
                    <img src="results_proj_5/1.7.im1.png" alt="Test Im1">
                    <figcaption>Campanile</figcaption>
                </figure>
                <figure>
                    <img src="results_proj_5/1.7.im2.png" alt="Test Im2">
                    <figcaption>House</figcaption>
                </figure>
                <figure>
                    <img src="results_proj_5/1.7.im3.png" alt="Test Im3">
                    <figcaption>Pyramid</figcaption>
                </figure>
            </div>
            <div class="images-row">
                <figure>
                    <img src="results_proj_5/1.7.im1_results.png" alt="Test Im1 Results">
                    <figcaption>Campanile Results</figcaption>
                </figure>
                <figure>
                    <img src="results_proj_5/1.7.im2_results.png" alt="Test Im2 Results">
                    <figcaption>House Results</figcaption>
                </figure>
                <figure>
                    <img src="results_proj_5/1.7.im3_results.png" alt="Test Im3 Results">
                    <figcaption>Pyramid Results</figcaption>
                </figure>
            </div>

            <h3>1.7.1 Editing Hand-Drawn and Web Images</h3>
            <p>
                In this step, I experimented with editing non-realistic images, such as hand-drawn 
                sketches and web images, using the same diffusion model procedure. By applying noise 
                and denoising iteratively, I was able to project these images onto the natural image
                manifold, transforming them into more realistic versions.
                I started by downloading a web image and applying the denoising 
                edits at varying noise levels. I also drew my own hand-drawn images, 
                following the same process to see how the model refined them. The 
                results showed how the diffusion model can creatively enhance and modify
                both hand-drawn and web-based images, gradually making them look more
                natural and realistic.

            </p>
            <div class="images-row">
                <figure>
                    <img src="results_proj_5/1.7.1.web.png" alt="Web">
                    <figcaption>Web Image: Venice Painting</figcaption>
                </figure>
                <figure>
                    <img src="results_proj_5/1.7.1.web_results.png" alt="Web Results">
                    <figcaption>Results</figcaption>
                </figure>
            </div>
            <div class="images-row">
                <figure>
                    <img src="results_proj_5/1.7.1.hand_im1.png" alt="Painting 1">
                    <figcaption>Hand-Drawn 1</figcaption>
                </figure>
                <figure>
                    <img src="results_proj_5/1.7.1.hand_im1_results.png" alt="Painting 1 Results">
                    <figcaption>Results</figcaption>
                </figure>
            </div>
            <div class="images-row">
                <figure>
                    <img src="results_proj_5/1.7.1.hand_im2.png" alt="Painting 2">
                    <figcaption>Hand-Drawn 2</figcaption>
                </figure>
                <figure>
                    <img src="results_proj_5/1.7.1.hand_im2_results.png" alt="Painting 2 Results">
                    <figcaption>Results</figcaption>
                </figure>
            </div>

            <h3>1.7.2 Inpainting</h3>
            <p>
                In this step, I implemented inpainting, which allows us to modify specific 
                parts of an image while preserving the rest. Using a binary mask, I applied 
                the diffusion model to create new content wherever the mask is active (set to 1), 
                while leaving the other areas of the image untouched.
                To do this, I started with the original image and the mask, running the diffusion model 
                denoising loop. At each step, I ensured that the pixels outside the mask were replaced with
                new content, while maintaining the existing content where the mask was 0. I used this 
                technique to inpaint the top of the Campanile and applied similar edits to my own images, 
                experimenting with different masks to create unique results.


            </p>
            <div class="images-row">
                <figure>
                    <img src="results_proj_5/1.7.2.mask1.png" alt="Mask1">
                    <figcaption>Campanile Mask</figcaption>
                </figure>
                <figure>
                    <img src="results_proj_5/1.7.2.mask1_results.png" alt="Results">
                    <figcaption>Results</figcaption>
                </figure>
            </div>
            <div class="images-row">
                <figure>
                    <img src="results_proj_5/1.7.2.mask2.png" alt="Mask 2">
                    <figcaption>Pyramid Mask</figcaption>
                </figure>
                <figure>
                    <img src="results_proj_5/1.7.2.mask2_results.png" alt="Results">
                    <figcaption>Results</figcaption>
                </figure>
            </div>
            <div class="images-row">
                <figure>
                    <img src="results_proj_5/1.7.2.mask3.png" alt="Mask 3">
                    <figcaption>House Mask</figcaption>
                </figure>
                <figure>
                    <img src="results_proj_5/1.7.2.mask3_results.png" alt="Results">
                    <figcaption>Results</figcaption>
                </figure>
            </div>

            <h3>1.7.3 Text Conditioned Image-to-Image Translation</h3>
            <p>
                In this step, I extended the inpainting process by incorporating text prompts, enabling more controlled edits.
            </p>
            <img src="results_proj_5/1.7.3.im1.png" alt="Rocket">
            <figcaption>"a rocket ship"</figcaption>
            <img src="results_proj_5/1.7.3.im3.png" alt="Rocket">
            <figcaption>"a pencil"</figcaption>
            <img src="results_proj_5/1.7.3.im2.png" alt="Hat">
            <figcaption>"a man wearing a hat"</figcaption>
        
            <h3>1.8 Visual Anagrams</h3>
            <p>
                In this part of the project, I implemented Visual Anagrams, using 
                diffusion models to generate optical illusions. The idea was to create
                an image that, when viewed in one orientation, portrays one scene, but
                when flipped upside down, reveals an entirely different scene.
                For this, I used two text prompts: one for the original image 
                and one for the flipped image. The process involved denoising 
                the image normally for one prompt and denoising the flipped 
                version of the image for the second prompt. By averaging the 
                noise estimates from both steps, I achieved a hybrid image that
                displays different scenes depending on its orientation.

            </p>
            <div class="images-row">
                <figure>
                    <img src="results_proj_5/1.8.im1.png" alt="Im1">
                    <figcaption>'an oil painting of an old man' and 'an oil painting of people around a campfire'</figcaption>
                </figure>
                <figure>
                    <img src="results_proj_5/1.8.im2.png" alt="Im2">
                    <figcaption>'a man wearing a hat' and 'an oil painting of a snowy mountain village'</figcaption>
                </figure>
                <figure>
                    <img src="results_proj_5/1.8.im3.png" alt="Im3">
                    <figcaption>'a photo of a hipster barista' and 'a photo of a dog'</figcaption>
                </figure>
            </div>

            <h3>1.10 Hybrid Images</h3>
            <p>
                In this part of the project, I implemented Hybrid Images using diffusion models. 
                The goal was to create images that reveal different scenes depending on the 
                viewer's distance. This technique combines low and high-frequency components 
                from two different prompts to produce a composite image with distinct features 
                at varying scales.
                The process involved using two separate text prompts and obtaining their 
                noise estimates using a diffusion model. For one prompt, the low-frequency 
                components were preserved, while for the other, the high-frequency components
                 were retained. By combining these components, I generated an image that 
                 looks like one thing from a distance and transforms into something 
                 entirely different when viewed up close.
            </p>
            <div class="images-row">
                <figure>
                    <img src="results_proj_5/1.10.im1.png" alt="Im1">
                    <figcaption>Looks like a skull from far away but a waterfall from close up</figcaption>
                </figure>
                <figure>
                    <img src="results_proj_5/1.10.im2.png" alt="Im2">
                    <figcaption>Looks like "a rocket ship" from far away but 'a photo of the amalfi cost' from close up</figcaption>
                </figure>
                <figure>
                    <img src="results_proj_5/1.10.im3.png" alt="Im3">
                    <figcaption>Looks like 'an oil painting of a snowy mountain village' from far away but 'an oil painting of people around a campfire' from close up</figcaption>
                </figure>
            </div>
        </section>

        <section>
            <h2>Part B: Training Diffusion Models from Scratch</h2>
            <h3>Overview</h3>
            <p>In part B, I trained my own diffusion model on the MNIST.</p>

            <h3>Part 1: Training a Single-Step Denoising UNet</h3>

            <h3>1.1 Implementing the UNet</h3>
            <p>In this part of the project, I implemented the UNet architecture 
                for image denoising in diffusion models.
            </p>
            <img src="results_proj_5/b_1.1.png" alt="UNet Architecture Visualization">

            <h3>1.2 Using the UNet to Train a Denoiser</h3>
            <p>
                We aim to solve the following denoising problem: Given a noisy image z, we aim to train a denoiser 
                Dθ such that it maps z to a clean image x. 
                z=x+oe, where e~N(0,I)
                Here we can visualize the different denoising processes over o = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]
            </p>
            <img src="results_proj_5/b_1.2.png" alt="Different noises">

            <h3>1.2.1 Training</h3>
            <p>
                I trained a UNet model to denoise images by mapping noisy inputs to clean versions using the MNIST dataset. 
                The model was trained for 5 epochs. I used the Adam optimizer, a hidden dimension of 128 and batch size of 256.
            </p>
            <h2>Training Loss Curve</h2>
            <img src="results_proj_5/b_1.2_training_curve.png" alt="Training Loss Curve">
            <div class="images-row">
                <figure>
                    <img src="results_proj_5/b_1.2.1_epoch.png" alt="Epoch 1">
                    <figcaption>Results on digits from the test set after 1 epoch of training</figcaption>
                </figure>
                <figure>
                    <img src="results_proj_5/b_1.2_5epoch.png" alt="Epoch 5">
                    <figcaption>Results on digits from the test set after 5 epochs of training</figcaption>
                </figure>
            </div>
            <h3>1.2.2 Out-of-Distribution Testing</h3>
            <p>
                See how the denoiser performs on different σ's that it wasn't trained for.
            </p>
            <img src="results_proj_5/b_1.2_out_of_bounds.png" alt="Out of Bounds">
            
            <h3>Part 2: Training a Diffusion Model</h3>
            <p>
                In this section, we changed our UNet to predict the added noise ϵ instead of the clean image x.

                First I worked on training a diffusion model using a time-conditioned UNet to gradually 
                denoise images and generate realistic samples. Later I introduced class-conditioning to 
                give more control over the generated images. 

            </p>

            <h3>2.1 Adding Time Conditioning to UNet</h3>
            <p>
                I modified the UNet to include time conditioning, embedding the timestep into the network with FCBlocks. 
            </p>
            <img src="results_proj_5/b_2.1_im.png" alt="Explanation">

            <h3>2.2 Training the UNet</h3>
            <p>
                The time-conditioned UNet was trained on MNIST, learning to predict noise from noisy images. 
                I used the Adam optimizer and exponential learning rate decay, batch size of 128, and a hidden 
                dimension of 64. I trained the model over 20 epochs.
            </p>
            <h2>Time-Conditioned UNet training loss curve</h2>
            <img src="results_proj_5/b_2.2_training_curve.png" alt="Training Curve">

            <h3>2.3 Sampling from the UNet</h3>
            <p>
                Starting with pure noise, the trained UNet iteratively denoised images to produce realistic MNIST digits.
            </p>
            <div class="images-row">
                <figure>
                    <img src="results_proj_5/b_2.3_sample_epoc5.png" alt="Epoch 5">
                    <figcaption>Epoch 5</figcaption>
                </figure>
                <figure>
                    <img src="results_proj_5/b_2.3_sample_epoch20.png" alt="Epoch 20">
                    <figcaption>Epoch 20</figcaption>
                </figure>
            </div>

            <h3>2.4 Adding Class-Conditioning to UNet</h3>
            <p>
                To control which digit was generated, I extended the UNet with class conditioning. 
                This involved adding one-hot encoded class vectors (0-9) into the network, with dropout 
                for occasional unconditional generation.
            </p>
            <h2>Class-conditioned UNet training loss curve</h2>
            <img src="results_proj_5/b_2.4_training_curve.png" alt="Training Curve">

            <h3>2.5 Sampling from the Class-Conditioned UNet</h3>
            <p>
                The class-conditioned UNet generated specific digits with improved accuracy using classifier-free guidance.
            </p>
            <div class="images-row">
                <figure>
                    <img src="results_proj_5/b_2.5_sample_epoch5.png" alt="Epoch 5">
                    <figcaption>Epoch 5</figcaption>
                </figure>
                <figure>
                    <img src="results_proj_5/b_2.5_sample_epoch20.png" alt="Epoch 20">
                    <figcaption>Epoch 20</figcaption>
                </figure>
            </div>
        </section>
    </main>
</body>
</html>
