<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fun With Diffusion Models!</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
            color: #333;
        }
        header {
            background: #2f2e2e;
            color: #ffffff;
            padding: 10px 20px;
            text-align: center;
        }
        main {
            padding: 20px;
            max-width: 900px;
            margin: 0 auto;
            background: #fff;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            margin-top: 20px;
        }
        section {
            margin-bottom: 20px;
        }
        h1, h2, h3 {
            color: #444;
        }
        img {
            display: block;
            max-width: 100%;
            height: auto;
            margin: 10px 0;
        }
        code {
            background: #f4f4f4;
            padding: 2px 4px;
            border-radius: 4px;
            font-family: Consolas, monospace;
        }
        .images-row {
            display: flex;
            justify-content: center;
            gap: 20px;
            margin: 20px 0;
        }
    </style>
</head>
<body>
    <header>
        <h1>Fun With Diffusion Models!</h1>
        <p>By: Daniela Fajardo</p>
    </header>
    <main>
        <section>
            <h2>Overview</h2>
            <p>In this project, I implemented and deployed diffusion models for image generation.</p>
        </section>
        <section>
            <h2>Part A: Diffusion Models Exploration</h2>
            <h3>Overview</h3>
            <p>In part A, I explored diffusion models, implemented sampling loops, and applied them for tasks such as inpainting and creating optical illusions.</p>

            <h3>Part 0: Image Generation with DeepFloyd IF</h3>
            <p>My seed is <strong>3037150359</strong>.</p>
            <p>
                Using the DeepFloyd IF diffusion model, a two stage Diffusion Model trained by Stability AI, 
                I generated 3 images for each one of the 3 provided text prompts. I varied the number of inference  
                steps from 5, 10, 20 and 40 for each one of  the images. Below are the results.

                The generated images align with the provided prompts. 
                We can  see that as num_inference_steps increases, 
                the images become more detailed and “artistic”. 

            </p>
            
            <div class="images-row">
                <figure>
                    <img src="results_proj_5/num_inferences_5.png" alt="Inference Steps 5">
                    <figcaption>Inference Steps 5</figcaption>
                </figure>
                <figure>
                    <img src="results_proj_5/num_inferences_10.png" alt="Inference Steps 10">
                    <figcaption>Inference Steps 10</figcaption>
                </figure>
            </div>
            <div class="images-row">
                <figure>
                    <img src="results_proj_5/num_inferences_20.png" alt="Inference Steps 20">
                    <figcaption>Inference Steps 20</figcaption>
                </figure>
                <figure>
                    <img src="results_proj_5/num_inferences_40.png" alt="Inference Steps 40">
                    <figcaption>Inference Steps 40</figcaption>
                </figure>
            </div>

            <h3>1.1 Forward Process</h3>
            <p>
                In this step, I implemented the forward process of diffusion, which adds scaled noise 
                to a clean image at different timesteps. Using a test image, I visualized the results 
                for t=250,500,750t = 250, 500, 750t=250,500,750, showing how the image becomes progressively 
                noisier as t increases. These results demonstrate the gradual transformation of a clean image 
                into noise, setting the stage for the reverse process to reconstruct the original image.
            </p>
            <img src="results_proj_5/1.1.im.png" alt="Forward Process Visualization">

            <h3>1.2 Classical Denoising</h3>
            <p>I used Gaussian Blur to try to remove the noise</p>
            <div class="images-row">
                <figure>
                    <img src="results_proj_5/1.2.250_im.png" alt="Gaussian Blur 250">
                    <figcaption>Noisy Campanile at t=250</figcaption>
                </figure>
                <figure>
                    <img src="results_proj_5/1.2.500_im.png" alt="Gaussian Blur 500">
                    <figcaption>Noisy Campanile at t=500</figcaption>
                </figure>
                <figure>
                    <img src="results_proj_5/1.2.750_im.png" alt="Gaussian Blur 750">
                    <figcaption>Noisy Campanile at t=750</figcaption>
                </figure>
            </div>

            <h3>1.3 One-Step Denoising</h3>
            <p>In this step, I utilized a pretrained UNet from the diffusion model 
                to denoise images. Using the noisy images generated in Part 1.2 
                (t=250,500,750t = 250, 500, 750t=250,500,750), I:
                1. Estimated the Gaussian noise present in the noisy images using the UNet.
                2. Removed the noise to reconstruct an estimate of the original image.

                I visualized the results for each timestep, showing the progression from the clean 
                original image to the noisy version and then back to the denoised estimate. This 
                demonstrates the model's ability to recover meaningful details from noisy inputs, 
                providing a closer approximation to the original image.
                </p>
            <div class="images-row">
                <figure>
                    <img src="results_proj_5/1.3.im1.png" alt="250">
                </figure>
                <figure>
                    <img src="results_proj_5/1.3.im2.png" alt="500">
                </figure>
                <figure>
                    <img src="results_proj_5/1.3.im3.png" alt="Gaussian Blur 750">
                </figure>
            </div>

            <h3>1.4 Iterative Denoising</h3>
            <p>
                In this step, I worked on iterative denoising, a powerful technique used in diffusion 
                models to clean up noisy images step by step. Starting with a heavily distorted image,
                 I gradually refined it over multiple steps, following a schedule of decreasing noise 
                 levels. This approach proved much better at reconstructing detailed, high-quality 
                 images compared to simpler methods like single-step denoising or blurring.
            </p>
            <img src="results_proj_5/1.4.im.png" alt="Iterative Denoising Results">

            <h3>1.5 Diffusion Model Sampling</h3>
            <p>
                In this step, I used the iterative denoising function to generate images from scratch 
                by starting with random noise. By setting the starting point to pure noise and 
                applying the denoising process iteratively, I was able to sample images corresponding 
                to the prompt "a high quality photo."
            </p>
            <img src="results_proj_5/1.5.im.png" alt="Generated Images">

            <h3>1.6 Classifier-Free Guidance</h3>
            <p>In this step, I implemented Classifier-Free Guidance (CFG) to significantly 
                enhance the quality of images generated from random noise. CFG works by 
                combining two noise estimates: one conditioned on a text prompt and another 
                unconditional estimate. By adjusting the balance between these estimates, 
                we can control the image quality.
                The images generated from scratch with the prompt "a high quality photo" and 
                CFG showed much higher quality compared to the previous section.
                </p>
            <img src="results_proj_5/1.6.im.png" alt="Classifier-Free Guidance Results">

            <h3>1.7 Image-to-Image Translation</h3>
            <p>Progressive refinement of noisy images:</p>
            <div class="images-row">
                <figure>
                    <img src="results_proj_5/1.7.im1.png" alt="Test Im1">
                    <figcaption>Campanile</figcaption>
                </figure>
                <figure>
                    <img src="results_proj_5/1.7.im2.png" alt="Test Im2">
                    <figcaption>House</figcaption>
                </figure>
                <figure>
                    <img src="results_proj_5/1.7.im3.png" alt="Test Im3">
                    <figcaption>Pyramid</figcaption>
                </figure>
            </div>
            <div class="images-row">
                <figure>
                    <img src="results_proj_5/1.7.im1_results.png" alt="Test Im1 Results">
                    <figcaption>Campanile Results</figcaption>
                </figure>
                <figure>
                    <img src="results_proj_5/1.7.im2_results.png" alt="Test Im2 Results">
                    <figcaption>House Results</figcaption>
                </figure>
                <figure>
                    <img src="results_proj_5/1.7.im3_results.png" alt="Test Im3 Results">
                    <figcaption>Pyramid Results</figcaption>
                </figure>
            </div>
        </section>

        <section>
            <h2>Part B: Training Diffusion Models from Scratch</h2>
            <h3>Overview</h3>
            <p>In part B, I trained my own diffusion model on the MNIST dataset.</p>

            <h3>1.1 Implementing the UNet</h3>
            <p>The UNet architecture was implemented for image denoising:</p>
            <img src="image_placeholder" alt="UNet Architecture Visualization">

            <h3>1.2 Training a Single-Step Denoiser</h3>
            <p>Training results over 5 epochs:</p>
            <p>Training Loss Curve:</p>
            <img src="image_placeholder" alt="Training Loss Curve">
            <p>Results after 5 epochs:</p>
            <img src="image_placeholder" alt="Denoised Images">

            <h3>2.2 Training a Time-Conditioned Diffusion Model</h3>
            <p>Time-conditioned UNet trained over 20 epochs:</p>
            <img src="image_placeholder" alt="Time-Conditioned UNet Loss Curve">
            <p>Generated samples:</p>
            <img src="image_placeholder" alt="Generated Samples">

            <h3>2.4 Adding Class Conditioning</h3>
            <p>Class-conditioned UNet results with guided image generation:</p>
            <img src="image_placeholder" alt="Class-Conditioned Results">
        </section>
    </main>
</body>
</html>
